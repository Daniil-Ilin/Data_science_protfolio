{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we consider the problem of utility maximizatio...</td>\n",
       "      <td>on optimal investment with processes of long o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in this paper we provide an explicit formula f...</td>\n",
       "      <td>boolean complexes for ferrers graphs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kinesin-5, also known as eg5 in vertebrates is...</td>\n",
       "      <td>relative velocity of sliding of microtubules b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we discuss the transition paths in a coupled b...</td>\n",
       "      <td>bifurcation of transition paths induced by cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>two types of room temperature detectors of ter...</td>\n",
       "      <td>all-electric detectors of the polarization sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  we consider the problem of utility maximizatio...   \n",
       "1  in this paper we provide an explicit formula f...   \n",
       "2  kinesin-5, also known as eg5 in vertebrates is...   \n",
       "3  we discuss the transition paths in a coupled b...   \n",
       "4  two types of room temperature detectors of ter...   \n",
       "\n",
       "                                               title  \n",
       "0  on optimal investment with processes of long o...  \n",
       "1               boolean complexes for ferrers graphs  \n",
       "2  relative velocity of sliding of microtubules b...  \n",
       "3  bifurcation of transition paths induced by cou...  \n",
       "4  all-electric detectors of the polarization sta...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate article names\n",
    "def generate_article_name(article_text, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(article_text, return_tensors=\"pt\")\n",
    "    # Generate names\n",
    "    output = model.generate(input_ids, max_length=128, top_p=0.95, do_sample=True, pad_token_id=128)\n",
    "    text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article discusses the latest research on artificial intelligence and its potential applications in the field of medicine. TL;DR: You can read a lot about AI using various different methods that we discussed.\n",
      "\n",
      "This article also discusses the latest research on Artificial Intelligence and its potential applications in the field of medicine. TL;DR: You can read a lot about AI using various different methods that we discussed.\n",
      "\n",
      "What will be a novel method of artificial intelligence being used will be the \"brain scan\", as discussed in this article.\n",
      "\n",
      "What will be a novel method of artificial intelligence being used will be the \"brain scan\", as discussed in\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "article_text = \"This article discusses the latest research on artificial intelligence and its potential applications in the field of medicine. TL;DR\"\n",
    "article_name = generate_article_name(article_text, model, tokenizer)\n",
    "print(article_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df.abstract.values\n",
    "titles = df.title.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\danii\\PycharmProjects\\pythonProject\\kaggle\\title_generation\\generate_names.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danii/PycharmProjects/pythonProject/kaggle/title_generation/generate_names.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m input_ids \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mencode(article, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m article \u001b[39min\u001b[39;00m articles]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danii/PycharmProjects/pythonProject/kaggle/title_generation/generate_names.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m labels \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mencode(title, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m title \u001b[39min\u001b[39;00m titles]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/danii/PycharmProjects/pythonProject/kaggle/title_generation/generate_names.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m dataset \u001b[39m=\u001b[39m TensorDataset(input_ids, labels)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danii/PycharmProjects/pythonProject/kaggle/title_generation/generate_names.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m data_loader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/danii/PycharmProjects/pythonProject/kaggle/title_generation/generate_names.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Set up the optimizer and scheduler\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danii\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:184\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[1;34m(self, *tensors)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 184\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39;49m(tensors[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m) \u001b[39m==\u001b[39;49m tensor\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m tensor \u001b[39min\u001b[39;49;00m tensors), \u001b[39m\"\u001b[39m\u001b[39mSize mismatch between tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    185\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors\n",
      "File \u001b[1;32mc:\\Users\\danii\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:184\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 184\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(tensors[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m tensor\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors), \u001b[39m\"\u001b[39m\u001b[39mSize mismatch between tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    185\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "# Assume that `articles` is a list of strings, each representing the text of a scientific article\n",
    "# and `titles` is a list of corresponding titles\n",
    "input_ids = [tokenizer.encode(article, return_tensors=\"pt\") for article in articles]\n",
    "labels = [tokenizer.encode(title, return_tensors=\"pt\") for title in titles]\n",
    "dataset = TensorDataset(input_ids, labels)\n",
    "data_loader = DataLoader(dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "# Set up the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=-1)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(1, 5):\n",
    "    total_loss = 0\n",
    "    for input_ids, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, labels=labels)\n",
    "        loss = logits[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Prepare the data\n",
    "# Assume that `articles` is a list of strings, each representing the text of a scientific article\n",
    "# and `titles` is a list of corresponding titles\n",
    "input_ids = [tokenizer.encode(article, return_tensors=\"pt\") for article in articles]\n",
    "labels = [[1] for _ in range(len(articles))] # 1 for title, 0 for non-title\n",
    "dataset = TensorDataset(input_ids, labels)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Set up the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=-1)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(1, 5):\n",
    "    total_loss = 0\n",
    "    for input_ids, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, labels=labels)\n",
    "        loss = logits[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Load the RuBERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"DeepPavlov/rubert-base-cased\", num_labels=2)\n",
    "\n",
    "# Prepare the data\n",
    "# Assume that `articles` is a list of strings, each representing the text of a scientific article\n",
    "# and `titles` is a list of corresponding titles\n",
    "input_ids = [tokenizer.encode(article, return_tensors=\"pt\") for article in articles]\n",
    "labels = [[1] for _ in range(len(articles))] # 1 for title, 0 for non-title\n",
    "dataset = TensorDataset(input_ids, labels)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Set up the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=-1)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(1, 5):\n",
    "    total_loss = 0\n",
    "    for input_ids, labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, labels=labels)\n",
    "        loss = logits[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_article_name(article_text, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(article_text, return_tensors=\"pt\")\n",
    "    logits = model(input_ids)[0]\n",
    "    # Get the index of the highest probability\n",
    "    max_index = torch.argmax(logits).item()\n",
    "    # Get the corresponding token\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([max_index])[0]\n",
    "    return predicted_token\n",
    "\n",
    "# Test the function\n",
    "article_text = \"This article discusses the latest research on artificial intelligence and its potential applications in the field of medicine.\"\n",
    "article_name = generate_article_name(article_text, model, tokenizer)\n",
    "print(article_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_article_name_rubert(article_text, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(article_text, return_tensors=\"pt\")\n",
    "    logits = model(input_ids)[0]\n",
    "    # Get the index of the highest probability\n",
    "    max_index = torch.argmax(logits).item()\n",
    "    # Get the corresponding token\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([max_index])[0]\n",
    "    return predicted_token\n",
    "\n",
    "# Test the function\n",
    "article_text = \"Эта статья обсуждает последние исследования в области искусственного интеллекта и его возможные приложения в медицине.\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "article_name = generate_article_name_rubert(article_text, model, tokenizer)\n",
    "print(article_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_article_name_rubert(article_text, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(article_text, return_tensors=\"pt\")\n",
    "    logits = model(input_ids)[0]\n",
    "    # Get the index of the highest probability\n",
    "    max_index = torch.argmax(logits).item()\n",
    "    # Get the corresponding token\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([max_index])[0]\n",
    "    # Decode the token\n",
    "    decoded_title = tokenizer.decode(predicted_token)\n",
    "    return decoded_title\n",
    "\n",
    "# Test the function\n",
    "article_text = \"Эта статья обсуждает последние исследования в области искусственного интеллекта и его возможные приложения в медицине.\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "article_name = generate_article_name_rubert(article_text, model, tokenizer)\n",
    "print(article_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated titles: ['This article explores the impact', 'This study examines the effects']\n",
      "Original titles: ['The Impact of Global Warming on the Environment', 'The Effects of Climate Change on the Arctic']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Pre-process data\n",
    "descriptions = [\"This article explores the impact of global warming on the environment.\",\"This study examines the effects of climate change on the Arctic.\"]\n",
    "titles = [\"The Impact of Global Warming on the Environment\",\"The Effects of Climate Change on the Arctic\"]\n",
    "\n",
    "# Convert strings to tokens\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "encodings = [tokenizer.encode(description, return_tensors='pt', max_length=64, truncation=True) for description in descriptions]\n",
    "\n",
    "# Load BART model\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Generate titles\n",
    "generated_titles = []\n",
    "for encoding in encodings:\n",
    "  outputs = model.generate(encoding, num_beams=4, min_length=4, max_length=16, early_stopping=True)\n",
    "  generated_titles.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Print generated titles\n",
    "print('Generated titles:', generated_titles)\n",
    "\n",
    "# Print original titles\n",
    "print('Original titles:', titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Pre-process data\n",
    "descriptions = [\"This article explores the impact of global warming on the environment.\",\"This study examines the effects of climate change on the Arctic.\"]\n",
    "titles = [\"The Impact of Global Warming on the Environment\",\"The Effects of Climate Change on the Arctic\"]\n",
    "\n",
    "# Convert strings to tokens\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "encodings = [tokenizer.encode(description, return_tensors='pt', max_length=64, truncation=True) for description in descriptions]\n",
    "targets = [tokenizer.encode(title, return_tensors='pt', max_length=32, truncation=True) for title in titles]\n",
    "\n",
    "# Load BART model\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Fine-tune the model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "for _ in range(10):\n",
    "  loss = 0\n",
    "  for encoding, target in zip(encodings, targets):\n",
    "    outputs = model(encoding, decoder_input_ids=target[:, :-1])\n",
    "    loss += outputs[0]\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "# Generate titles\n",
    "generated_titles = []\n",
    "for encoding in encodings:\n",
    "  outputs = model.generate(encoding, num_beams=4, max_length=32, early_stopping=True)\n",
    "  generated_titles.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Print generated titles\n",
    "print('Generated titles:', generated_titles)\n",
    "\n",
    "# Print original titles\n",
    "print('Original titles:', titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0189e139adb8fe9a1b0addd3ecae1eeec637ab14a24a849a747c9f35583bd08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
