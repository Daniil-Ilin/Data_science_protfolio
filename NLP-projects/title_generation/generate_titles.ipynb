{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install rouge_score"
      ],
      "metadata": {
        "id": "s6Q6hNPvJpMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "J0BzXum1JsUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "id": "-665imJPJu1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "ndLMuc8MKM1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh1OlLv8Joe2",
        "outputId": "80b6036e-2e1e-45e6-d443-9139df8c2b7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\danii\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Imoprt all necessary libraries\n",
        "import transformers\n",
        "from datasets import Dataset, DatasetDict\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB6av7y3Joe6"
      },
      "outputs": [],
      "source": [
        "# Load training data\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/Title_generation/train.csv')\n",
        "# Load rouge score\n",
        "metric = load('rouge')\n",
        "# Initialize path to model\n",
        "model_checkpoints = '/content/gdrive/MyDrive/Title_generation/my_model'\n",
        "# Load model's tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8u6E5SMJoe7",
        "outputId": "08ae716a-7016-43e4-d0f4-e23503bde103"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "186"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get max lenght of summary\n",
        "len(tokenizer.encode(data.abstract.max(), return_tensors='pt')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkN-zJ7EJoe7",
        "outputId": "45780e4d-cd60-44fe-9db1-534c2fabdcfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get max lenght of titles\n",
        "len(tokenizer.encode(data.title.max(), return_tensors='pt')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSOcV8uUJoe8"
      },
      "outputs": [],
      "source": [
        "# Split data into train and validation sets\n",
        "train = Dataset.from_pandas(data[:125000])\n",
        "val = Dataset.from_pandas(data[125000:])\n",
        "# Convert data sets into DatesetDict\n",
        "data = DatasetDict({'train':train, 'validation':val})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAtBf79BJoe8",
        "outputId": "ab7f7a39-f884-400e-9550-e8cc81bb730c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['abstract', 'title'],\n",
              "        num_rows: 120000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['abstract', 'title'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['abstract', 'title'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMIzxfaQJoe9"
      },
      "outputs": [],
      "source": [
        "# Set max input and target lenghts\n",
        "max_input = 256\n",
        "max_target = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCFAgVn5Joe9"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data_to_process):\n",
        "    # Get all texts\n",
        "    inputs = [text for text in data_to_process['abstract']]\n",
        "    # Tokenize them\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input, padding='max_length', truncation=True)\n",
        "    # With target tokenizer\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        # Tokenize all titles\n",
        "        targets = tokenizer(data_to_process['title'], max_length=max_target, padding='max_length', truncation=True)\n",
        "    # Replace lables in model inputs with targets ids\n",
        "    model_inputs['labels'] = targets['input_ids']\n",
        "    \n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywejm9zcJoe-",
        "outputId": "4153e34c-7b13-4fec-ecdd-83706547f391",
        "colab": {
          "referenced_widgets": [
            "a475c72ef42d4b2aaca98f2dc33818bd",
            "57caa549b2b9419d85474fc20f513925",
            "747ae25a84b94eaa9134cb0d521791d2"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a475c72ef42d4b2aaca98f2dc33818bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/120 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\danii\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57caa549b2b9419d85474fc20f513925",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "747ae25a84b94eaa9134cb0d521791d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Tokenize data\n",
        "tokenized_data = data.map(preprocess_data, batched=True, remove_columns=['abstract', 'title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKNtqBZxJoe-",
        "outputId": "f853dc57-f555-480d-ad55-bf444297c0d3",
        "colab": {
          "referenced_widgets": [
            "8ca19d98629d4f529da7185c4d286f83"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ca19d98629d4f529da7185c4d286f83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\danii\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\danii\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "# Download model\n",
        "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HStexp1Joe_"
      },
      "outputs": [],
      "source": [
        "# Initialize collator\n",
        "collator = transformers.DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSOBxZuSJoe_"
      },
      "outputs": [],
      "source": [
        "# Set batch size\n",
        "batch_size = 18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDENJ3hxJoe_"
      },
      "outputs": [],
      "source": [
        "def compute_rouge(pred):\n",
        "    # Get model prediction and target labels tokens\n",
        "    predictions, labels = pred\n",
        "    # Decode predictions and labels\n",
        "    decode_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decode_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Compute model's performance\n",
        "    res = metric.compute(predictions=decode_predictions, references=decode_labels, use_stemmer=True)\n",
        "    res = {key: value*100 for key, value in res.items()}\n",
        "\n",
        "    pred_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    res['gen_len'] = np.mean(pred_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k,v in res.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rzjwr49fJofA"
      },
      "outputs": [],
      "source": [
        "# Define model's arguments\n",
        "args = transformers.Seq2SeqTrainingArguments(\n",
        "    'conversation-summ',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    eval_accumulation_steps=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXn-YkKHJofA"
      },
      "outputs": [],
      "source": [
        "# Define trainer\n",
        "trainer = transformers.Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_data['train'],\n",
        "    eval_dataset=tokenized_data['validation'],\n",
        "    data_collator=collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_rouge\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQFAQS1PJofA"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNxj1KUtJofB"
      },
      "outputs": [],
      "source": [
        "# Save trained model\n",
        "trainer.save_model(\"/content/gdrive/MyDrive/Title_generation/my_model_2_epoch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25N9iBXpJofB"
      },
      "outputs": [],
      "source": [
        "# Load test data\n",
        "test_data = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdv7NRxkJofB"
      },
      "outputs": [],
      "source": [
        "# Get all texts\n",
        "texts = test_data.abstract.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpaKZ6X3JofB"
      },
      "outputs": [],
      "source": [
        "# For each text\n",
        "for i in range(len(texts)):\n",
        "    # Tokenize text\n",
        "    model_inputs = tokenizer(texts[i], max_length=max_input, padding='max_length', truncation=True)\n",
        "    # Generate title\n",
        "    pred, _, _ = trainer.predict([model_inputs])\n",
        "    # Replace text with decoded title\n",
        "    texts[i] = tokenizer.decode(pred[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR3p5a0WJofC"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "path = './my_model'\n",
        "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(path)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWJ0vNljJofC"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('test.csv')\n",
        "pred = pd.read_csv('predictions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZPF9yAOJofC"
      },
      "outputs": [],
      "source": [
        "abstracts = test.abstract.values\n",
        "titles = pred.abstract.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_OTvrSwJofC"
      },
      "outputs": [],
      "source": [
        "# Save orginal texts and generated titles into .csv format\n",
        "submission_df = pd.DataFrame({'abstract': abstracts, 'title': titles})\n",
        "submission_df.to_csv('predicted_titles.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yORwYVjKJofC"
      },
      "outputs": [],
      "source": [
        "# Generate kaggle submission\n",
        "import string\n",
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "\n",
        "def generate_csv(input_file='predicted_titles.csv',\n",
        "                 output_file='submission.csv',\n",
        "                 voc_file='vocs.pkl'):\n",
        "    '''\n",
        "    Generates file in format required for submitting result to Kaggle\n",
        "    \n",
        "    Parameters:\n",
        "        input_file (str) : path to csv file with your predicted titles.\n",
        "                           Should have two fields: abstract and title\n",
        "        output_file (str) : path to output submission file\n",
        "        voc_file (str) : path to voc.pkl file\n",
        "    '''\n",
        "    data = pd.read_csv(input_file)\n",
        "    with open(voc_file, 'rb') as voc_file:\n",
        "        vocs = pickle.load(voc_file)\n",
        "\n",
        "    with open(output_file, 'w') as res_file:\n",
        "        res_file.write('Id,Predict\\n')\n",
        "        \n",
        "    output_idx = 0\n",
        "    for row_idx, row in data.iterrows():\n",
        "        trg = row['title']\n",
        "        trg = trg.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
        "        trg.extend(['_'.join(ngram) for ngram in list(ngrams(trg, 2)) + list(ngrams(trg, 3))])\n",
        "        \n",
        "        VOCAB_stoi = vocs[row_idx]\n",
        "        trg_intersection = set(VOCAB_stoi.keys()).intersection(set(trg))\n",
        "        trg_vec = np.zeros(len(VOCAB_stoi))    \n",
        "\n",
        "        for word in trg_intersection:\n",
        "            trg_vec[VOCAB_stoi[word]] = 1\n",
        "\n",
        "        with open(output_file, 'a') as res_file:\n",
        "            for is_word in trg_vec:\n",
        "                res_file.write('{0},{1}\\n'.format(output_idx, int(is_word)))\n",
        "                output_idx += 1\n",
        "\n",
        "\n",
        "generate_csv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUNwqJdMJofD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b0189e139adb8fe9a1b0addd3ecae1eeec637ab14a24a849a747c9f35583bd08"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
