{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train and test data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Initialize function to remove URLs\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "# Initialize function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "# Load english stopwords\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "# Initialize function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proccessing data\n",
    "# Remove URLs, punctuation and stopwords from training data\n",
    "train['text'] = train['text'].map(remove_URL)\n",
    "train['text'] = train['text'].map(remove_punct)\n",
    "train['text'] = train['text'].map(remove_stopwords)\n",
    "# Remove URLs, punctuation and stopwords from testing data\n",
    "test['text'] = test['text'].map(remove_URL)\n",
    "test['text'] = test['text'].map(remove_punct)\n",
    "test['text'] = test['text'].map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "df = pd.concat([train, test], ignore_index=True)\n",
    "counter = counter_word(df['text'])\n",
    "num_unique_words = len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train['text'], train['target'], test_size=0.33, random_state=42)\n",
    "X_test = test['text'].to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "# create tokenizer\n",
    "token = Tokenizer(num_words = num_unique_words)\n",
    "token.fit_on_texts(X_train)\n",
    "# tokenize X_train, X_val and X_test\n",
    "X_train_seq = token.texts_to_sequences(X_train)\n",
    "X_val_seq = token.texts_to_sequences(X_val)\n",
    "X_test_seq = token.texts_to_sequences(X_test)\n",
    "# pad X_train, X_val and X_test\n",
    "X_train_pad = sequence.pad_sequences(X_train_seq , maxlen = max_len)\n",
    "X_val_pad = sequence.pad_sequences(X_val_seq , maxlen = max_len)\n",
    "X_test_pad = sequence.pad_sequences(X_test_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 50, 32)            725536    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                24832     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 250)               16250     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 250)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 120)               30120     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 120)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 121       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 796,859\n",
      "Trainable params: 796,859\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(num_unique_words , 32 , input_length = max_len),\n",
    "    layers.LSTM(64),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(250, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'RMSprop' , metrics = 'accuracy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40/40 [==============================] - 4s 51ms/step - loss: 0.6504 - accuracy: 0.6235 - val_loss: 0.5354 - val_accuracy: 0.7545\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 0.3878 - accuracy: 0.8412 - val_loss: 0.4674 - val_accuracy: 0.7875\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 0.2641 - accuracy: 0.9012 - val_loss: 0.5003 - val_accuracy: 0.7927\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.1833 - accuracy: 0.9351 - val_loss: 0.6027 - val_accuracy: 0.7799\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.1349 - accuracy: 0.9543 - val_loss: 0.6668 - val_accuracy: 0.7772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21e4c6fe620>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad,y_train,batch_size=128,epochs=5,\n",
    "          validation_data=(X_val_pad, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0189e139adb8fe9a1b0addd3ecae1eeec637ab14a24a849a747c9f35583bd08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
