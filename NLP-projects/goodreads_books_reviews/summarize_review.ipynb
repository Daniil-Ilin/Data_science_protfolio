{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Set device to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained model for text summarization\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "bart_model.to(device)\n",
    "# Load BART tokenizer\n",
    "bart_tok = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "def summarize(sentences):\n",
    "    # Define function for single text summarization\n",
    "    def single_summarise(input_sent_ids):\n",
    "        # Generate summary\n",
    "        summary_ids = bart_model.generate(input_sent_ids, max_length=510, num_beams=4, early_stopping=True)\n",
    "        # Decode summary back to text\n",
    "        summary_text = bart_tok.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        # Return summarized text\n",
    "        return summary_text\n",
    "\n",
    "    # Define function for summarization of too large texts\n",
    "    def large_summarize(input_ids):\n",
    "        # Initialize result as an empty tensor\n",
    "        summ = torch.tensor([], dtype=torch.long).to(device)\n",
    "        # While there is more then 1024 ids in input\n",
    "        while len(input_ids[0])>1024:\n",
    "            # Get first 1023 ids and add SEP token at the end\n",
    "            part = torch.cat((input_ids[0][:1023].unsqueeze(-2), torch.tensor([2]).unsqueeze(-2).to(device)), dim=-1)\n",
    "            # Generate summary of this firs 1024 tokens\n",
    "            sum_part = bart_model.generate(part, max_length=510, num_beams=4, early_stopping=True)\n",
    "            # Add summary to the result\n",
    "            summ = torch.cat((summ, sum_part), dim=-1)\n",
    "            # Remove summurized ids and add [CLS] in the beggining\n",
    "            input_ids = torch.cat((torch.tensor([0]).unsqueeze(-2).to(device), input_ids[0][1023:].unsqueeze(-2)), dim=-1)\n",
    "        # After loop is done, get summary of the last ids\n",
    "        sum_part = bart_model.generate(input_ids, max_length=510, num_beams=4, early_stopping=True)\n",
    "        # And add it to the result\n",
    "        summ = torch.cat((summ, sum_part), dim=-1)\n",
    "        # Return the result\n",
    "        return summ\n",
    "\n",
    "    # For every sentence...\n",
    "    for i in range(len(sentences)):\n",
    "        if i%10000 == 0:\n",
    "            print(f'Processed over {i} elements')\n",
    "\n",
    "        # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "        input_ids = bart_tok.encode(sentences[i], return_tensors=\"pt\").clone().detach().requires_grad_(False).to(device)\n",
    "\n",
    "        # If sentence length is between 512 and 1024\n",
    "        if 512<len(input_ids[0])<=1024:\n",
    "            # Summarize sententce to max 510 tokens\n",
    "            sentences[i] = single_summarise(input_ids)\n",
    "        # If sentence is more then 1024 tokens\n",
    "        elif len(input_ids[0])>1024:\n",
    "            # While its more then 510 tokens\n",
    "            while len(input_ids[0])>510:\n",
    "                # Summurize it\n",
    "                input_ids = large_summarize(input_ids)\n",
    "            # After its summurized, decode it and update the sentence\n",
    "            sentences[i] = bart_tok.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return sentences            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0189e139adb8fe9a1b0addd3ecae1eeec637ab14a24a849a747c9f35583bd08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
