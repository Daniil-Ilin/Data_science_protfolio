{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# set a seed value\n",
    "torch.manual_seed(555)\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import transformers\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train test data\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# shuffle\n",
    "df = shuffle(df_train)\n",
    "\n",
    "# initialize kfold\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1024)\n",
    "\n",
    "# for stratification\n",
    "y = df['label']\n",
    "\n",
    "# Put the folds into a list. This is a list of tuples.\n",
    "fold_list = list(kf.split(df, y))\n",
    "\n",
    "train_df_list = []\n",
    "val_df_list = []\n",
    "\n",
    "for i, fold in enumerate(fold_list):\n",
    "\n",
    "    # map the train and val index values to dataframe rows\n",
    "    df_train = df[df.index.isin(fold[0])]\n",
    "    df_val = df[df.index.isin(fold[1])]\n",
    "    \n",
    "    train_df_list.append(df_train)\n",
    "    val_df_list.append(df_val)\n",
    "\n",
    "\n",
    "print(len(train_df_list))\n",
    "print(len(val_df_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'xlm-roberta-base'\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "NUM_FOLDS_TO_TRAIN = 3\n",
    "\n",
    "L_RATE = 1e-5\n",
    "MAX_LEN = 256\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "NUM_CORES = os.cpu_count()\n",
    "\n",
    "NUM_CORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader\n",
    "class CompDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, train_data=True):\n",
    "        self.df_data = df\n",
    "        self.train_data = train_data\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # get the sentence from the dataframe\n",
    "        sentence1 = self.df_data.loc[index, 'premise']\n",
    "        sentence2 = self.df_data.loc[index, 'hypothesis']\n",
    "\n",
    "        # Process the sentence\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                    sentence1, sentence2,           # Sentences to encode.\n",
    "                    add_special_tokens = True,      # Add the special tokens.\n",
    "                    max_length = MAX_LEN,           # Pad & truncate all sentences.\n",
    "                    pad_to_max_length = True,\n",
    "                    return_attention_mask = True,   # Construct attn. masks.\n",
    "                    return_tensors = 'pt',          # Return pytorch tensors.\n",
    "               )\n",
    "\n",
    "        # These are torch tensors.\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "\n",
    "        sample = (padded_token_list, att_mask)\n",
    "\n",
    "        if not self.train_data:\n",
    "            target = (torch.tensor(self.df_data.loc[index, 'label']),)\n",
    "            sample += target\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed.\n",
    "seed_val = 101\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists to store the val acc results.\n",
    "# The number of items in this list will correspond to\n",
    "# the number of folds that the model is being trained on.\n",
    "fold_val_acc_list = []\n",
    "for i in range(0, NUM_FOLDS):\n",
    "    \n",
    "    # append an empty list\n",
    "    fold_val_acc_list.append([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, NUM_EPOCHS):\n",
    "    \n",
    "    #print(\"\\nNum folds used for training:\", NUM_FOLDS_TO_TRAIN)\n",
    "    print(\"\\nNum folds used for training:\", NUM_FOLDS)\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, NUM_EPOCHS))\n",
    "    \n",
    "    # Get the number of folds\n",
    "    num_folds = len(train_df_list)\n",
    "\n",
    "    # For this epoch, store the val acc scores for each fold in this list.\n",
    "    # We will use this list to calculate the cv at the end of the epoch.\n",
    "    epoch_acc_scores_list = []\n",
    "    \n",
    "    #for fold_index in range(0, NUM_FOLDS_TO_TRAIN):\n",
    "    for fold_index in range(0, NUM_FOLDS):\n",
    "\n",
    "        print('\\n== Fold Model', fold_index)\n",
    "\n",
    "        # Load the fold model\n",
    "        if epoch == 0:\n",
    "            \n",
    "            # define the model\n",
    "            model = XLMRobertaForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels = 3,)\n",
    "                        \n",
    "            optimizer = AdamW(model.parameters(), lr = L_RATE, eps = 1e-8)\n",
    "            \n",
    "        else:\n",
    "            # Get the fold model\n",
    "            path_model = 'model_' + str(fold_index) + '.bin'\n",
    "            model.load_state_dict(torch.load(path_model))\n",
    "        \n",
    "        # Set up the train and val dataloaders\n",
    "        # Intialize the fold dataframes\n",
    "        df_train = train_df_list[fold_index]\n",
    "        df_val = val_df_list[fold_index]\n",
    "        \n",
    "        # Reset the indices or the dataloader won't work.\n",
    "        df_train = df_train.reset_index(drop=True)\n",
    "        df_val = df_val.reset_index(drop=True)\n",
    "    \n",
    "        # Create the dataloaders\n",
    "        train_data = CompDataset(df_train)\n",
    "        val_data = CompDataset(df_val)\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=NUM_CORES)\n",
    "\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_data,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=NUM_CORES)\n",
    "    \n",
    "        # TRAINING\n",
    "        stacked_val_labels = []\n",
    "        targets_list = []\n",
    "\n",
    "        print('Training...')\n",
    "\n",
    "        # put the model into train mode\n",
    "        model.train()\n",
    "\n",
    "        # This turns gradient calculations on and off.\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "            train_status = 'Batch ' + str(i+1) + ' of ' + str(len(train_dataloader))\n",
    "\n",
    "            print(train_status, end='\\r')\n",
    "\n",
    "            b_input_ids = batch[0]\n",
    "            b_input_mask = batch[1]\n",
    "            b_labels = batch[2]\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "            outputs = model(b_input_ids,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "            # Get the loss from the outputs tuple: (loss, logits)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Convert the loss from a torch tensor to a number.\n",
    "            # Calculate the total loss.\n",
    "            total_train_loss = total_train_loss + loss.item()\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Use the optimizer to update Weights\n",
    "            optimizer.step()\n",
    "        \n",
    "        print('Train loss:' ,total_train_loss)\n",
    "\n",
    "        # VALIDATION\n",
    "        print('\\nValidation...')\n",
    "\n",
    "        # Put the model in evaluation mode.\n",
    "        model.eval()\n",
    "\n",
    "        # Turn off the gradient calculations.\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_val_loss = 0\n",
    "\n",
    "\n",
    "        for j, val_batch in enumerate(val_dataloader):\n",
    "\n",
    "            val_status = 'Batch ' + str(j+1) + ' of ' + str(len(val_dataloader))\n",
    "\n",
    "            print(val_status, end='\\r')\n",
    "\n",
    "            b_input_ids = val_batch[0]\n",
    "            b_input_mask = val_batch[1]\n",
    "            b_labels = val_batch[2]    \n",
    "\n",
    "            outputs = model(b_input_ids,\n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "\n",
    "            # Get the loss from the outputs tuple: (loss, logits)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Convert the loss from a torch tensor to a number.\n",
    "            # Calculate the total loss.\n",
    "            total_val_loss = total_val_loss + loss.item()\n",
    "\n",
    "            # Get the preds\n",
    "            preds = outputs[1]\n",
    "\n",
    "            # Move preds to the CPU\n",
    "            val_preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            # Move the labels to the cpu\n",
    "            targets_np = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Append the labels to a numpy list\n",
    "            targets_list.extend(targets_np)\n",
    "\n",
    "            if j == 0:  # first batch\n",
    "                stacked_val_preds = val_preds\n",
    "\n",
    "            else:\n",
    "                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
    "\n",
    "        # Calculate the validation accuracy for this fold\n",
    "        y_true = targets_list\n",
    "        y_pred = np.argmax(stacked_val_preds, axis=1)\n",
    "\n",
    "        val_acc = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        epoch_acc_scores_list.append(val_acc)\n",
    "\n",
    "        print('Val loss:' ,total_val_loss)\n",
    "        print('Val acc: ', val_acc)\n",
    "        \n",
    "        # Save the best model\n",
    "        if epoch == 0:\n",
    "            \n",
    "            # Save the Model\n",
    "            model_name = 'model_' + str(fold_index) + '.bin'\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            print('Saved model as ', model_name)\n",
    "            \n",
    "        if epoch != 0:\n",
    "        \n",
    "            val_acc_list = fold_val_acc_list[fold_index]\n",
    "            best_val_acc = max(val_acc_list)\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                # save the model\n",
    "                model_name = 'model_' + str(fold_index) + '.bin'\n",
    "                torch.save(model.state_dict(), model_name)\n",
    "                print('Val acc improved. Saved model as ', model_name)\n",
    "                \n",
    "        # Save the val_acc for this fold model\n",
    "        fold_val_acc_list[fold_index].append(val_acc)\n",
    "        \n",
    "        # Use the garbage collector to save memory.\n",
    "        gc.collect()\n",
    "        \n",
    "\n",
    "    # Calculate the CV accuracy score over all folds in this epoch\n",
    "    # Print the average val accuracy for all 5 folds\n",
    "    cv_acc = sum(epoch_acc_scores_list)/NUM_FOLDS_TO_TRAIN\n",
    "    print(\"\\nCV Acc:\", cv_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracy scores for each fold model.\n",
    "fold_val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader\n",
    "\n",
    "test_data = CompDataset(df_test, train_data=False)\n",
    "\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=False,\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set\n",
    "print('\\nTest Set...')\n",
    "\n",
    "model_preds_list = []\n",
    "\n",
    "print('Total batches:', len(test_dataloader))\n",
    "\n",
    "for fold_index in range(0, NUM_FOLDS_TO_TRAIN):\n",
    "    \n",
    "    print('\\nFold Model', fold_index)\n",
    "\n",
    "    # Load the fold model\n",
    "    path_model = 'model_' + str(fold_index) + '.bin'\n",
    "    model.load_state_dict(torch.load(path_model))\n",
    "\n",
    "    stacked_val_labels = []\n",
    "\n",
    "    # Put the model in evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Turn off the gradient calculations.\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_val_loss = 0\n",
    "\n",
    "    for j, h_batch in enumerate(test_dataloader):\n",
    "\n",
    "        inference_status = 'Batch ' + str(j + 1)\n",
    "\n",
    "        print(inference_status, end='\\r')\n",
    "\n",
    "        b_input_ids = h_batch[0]\n",
    "        b_input_mask = h_batch[1]\n",
    "\n",
    "        outputs = model(b_input_ids,\n",
    "                attention_mask=b_input_mask)\n",
    "\n",
    "        # Get the preds\n",
    "        preds = outputs[0]\n",
    "\n",
    "        # Move preds to the CPU\n",
    "        val_preds = preds.detach().cpu().numpy()\n",
    "        \n",
    "        # Stack the predictions.\n",
    "        if j == 0:  # first batch\n",
    "            stacked_val_preds = val_preds\n",
    "\n",
    "        else:\n",
    "            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
    "        \n",
    "    model_preds_list.append(stacked_val_preds)\n",
    "    \n",
    "print('\\nPrediction complete.')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the predictions of all fold models\n",
    "for i, item in enumerate(model_preds_list):\n",
    "    \n",
    "    if i == 0:        \n",
    "        preds = item        \n",
    "    else:    \n",
    "        # Sum the matrices\n",
    "        preds = item + preds\n",
    "\n",
    "# Average the predictions\n",
    "avg_preds = preds/(len(model_preds_list))\n",
    "\n",
    "test_preds = np.argmax(avg_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample_submission file\n",
    "df_sample = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Assign the preds to the prediction column\n",
    "df_sample['prediction'] = test_preds\n",
    "\n",
    "# Create a submission csv file\n",
    "df_sample.to_csv('idv_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0189e139adb8fe9a1b0addd3ecae1eeec637ab14a24a849a747c9f35583bd08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
